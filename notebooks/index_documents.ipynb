{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install psycopg2\n",
    "# !pip install sqlalchemy\n",
    "# !pip install langchain\n",
    "# !pip install llama-index\n",
    "# !pip install llama-index-llms-huggingface\n",
    "# !pip install torch\n",
    "# !pip install llama-index-embeddings-langchain\n",
    "# !pip install bitsandbytes\n",
    "# !pip install sentence_transformers\n",
    "# %pip install llama-index-readers-web\n",
    "# %pip install llama-index-vector-stores-postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BAAI BGE model developed by the Beijing Academy of Artificial Intelligence (BAAI) is a state-of-the-art embedding model that is designed to encode text data into high-dimensional vector representations. This model is capable of capturing complex relationships and semantic knowledge within the data, making it a powerful tool for a wide range of natural language processing (NLP) tasks. In this blog we will use this mdodel to encode the text data into high-dimensional vector representations. Then at later stage we can use this encoded data to do a similarity search for a particular query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"BAAI/bge-large-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "# Create the embedding model using the HuggingFaceBgeEmbeddings class\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceBgeEmbeddings(model_name=embedding_model_name)\n",
    ")\n",
    "\n",
    "# Get the embedding dimension of the model by doing a forward pass with a dummy input\n",
    "embed_dim = len(embed_model.get_text_embedding(\"Hello world\")) # 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways of building a knowledge base. We can use the PDF/txt file (the most common ones) to create the knowledge base. We can also use the html files to create the knowledge base. We can also use the data from the web to create the knowledge base. In this blog, since we're building a chatbot for E2E Website to help users find the information they need easily, we will use the webpages directly to create the knowledge base. Llama-Index provides a simple way to create a document from a webpage. We're mainly going to use the homepage, product, about us, contact us, contact sales, t&c, privacy policy, FAQ pages to create the knowledge base. You can change this as per your requirement. Note that having a high-quality knowledge base is the key to the success of the chatbot. So a well curated PDF with just the essential information is the best way to create the knowledge base. But for the purpose of this blog, we will use the webpages directly to create the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\n",
    "        \"https://www.e2enetworks.com/\",\n",
    "        \"https://www.e2enetworks.com/products\",\n",
    "        \"https://www.e2enetworks.com/about-us\",\n",
    "        \"https://www.e2enetworks.com/contact-us\",\n",
    "        \"https://www.e2enetworks.com/contact-sales\",\n",
    "        \"https://www.e2enetworks.com/policies/service-level-agreement\",\n",
    "        \"https://www.e2enetworks.com/policies/terms-of-service\",\n",
    "        \"https://www.e2enetworks.com/policies/privacy-policy\",\n",
    "        \"https://www.e2enetworks.com/policies/refund-policy\",\n",
    "        \"https://www.e2enetworks.com/policy-faq\",\n",
    "        \"https://www.e2enetworks.com/countries-served\",\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, llama-index uses the OpenAI's LLM. But we do not want that, instead we want to use our own local LLM, Mixtral 8x7B. So, we need to change the settings of llama-index to use our local LLM and embeddings model. We also need to configure the settings for the creating of knowlege base. The most important settings are chunk_size and chunk_overlap. In our knowledge base, we can have a very long document, or content. Instead of indexing the whole document at once, which is not feasible, inefficient, slow, and leads to a bad performance, we divide the document into several chunks and index each chunk separately. This way, when we query the knowledge base, we can retrieve only the relevant chunks and not the whole document. This makes the retrieval process faster and more efficient. The chunk_size is the size of each chunk and the chunk_overlap is the overlap between the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "Settings.llm = None\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then setup the database. We use the PGVector from postgres as the database to store the knowledge base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"postgresql://postgres:test123@localhost:5432\"\n",
    "db_name = \"chatbotdb\"\n",
    "table_name = 'companyDocEmbeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(connection_string)\n",
    "# Set autocommit to True to avoid having to commit after every command\n",
    "conn.autocommit = True\n",
    "\n",
    "# Create the database\n",
    "# If it already exists, then delete it and create a new one\n",
    "with conn.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index the knowledge\n",
    "\n",
    "Once we've created the desired table in which we want to save the embeddings of the knowlege base, we'll start the process of indexing the documents. Here we use PGVector to store the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "# Creates a URL object from the connection string\n",
    "url = make_url(connection_string)\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=table_name,\n",
    "    embed_dim=embed_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "# Create the storage context to be used while indexing and storing the vectors\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00cd57144f84557b0389b4f668953bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a862da3ce4b04712a53e9168f22bba76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create the index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have the knowledge base indexed and ready to be queried. We can now use the encoded data to do a similarity search for a particular query. Let's delve into the second part of the blog where we will use the encoded data to do a similarity search for a particular query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quamer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
