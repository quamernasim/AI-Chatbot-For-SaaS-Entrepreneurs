{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "# model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "# # model_name = 'mistralai/Mistral-7B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86fa2f9693449409a7a1cf3601c8746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "# from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# Context Window specifies how many tokens to use as context for the LLM\n",
    "context_window = 4096\n",
    "# Max New Tokens specifies how many new tokens to generate for the LLM\n",
    "max_new_tokens = 256\n",
    "# Device specifies which device to use for the LLM\n",
    "device = \"cuda\"\n",
    "\n",
    "# This is the prompt that will be used to instruct the model behavior\n",
    "system_prompt = \"\"\"\n",
    "    You are an AI chatbot that is designed to answer questions related to E2E Networks. \n",
    "    You are provided with a context and a question. You need to answer the question based on the context provided. \n",
    "    If the context is not helpful, do not answer based on prior knowledge, instead, redirect the user to the E2E Networks Support team. \n",
    "    You should also provide links that you got from context that are relevant to the answer. \n",
    "    You are allowed to answer in first person only, like I/Us/Our; It should feel like a human is answering the question. \n",
    "    You should only provide the links and not like [E2E Networks Official Website](https://www.e2enetworks.com/)\n",
    "    You're not allowed to say something like \"Based on the context, I think the answer is...\", instead, you should directly answer the question.\n",
    "    When in confusion, you can ask for more information from the user.\n",
    "\n",
    "    Here is an example of how you should answer:\n",
    "\n",
    "    Question: What is the pricing for E2E Networks?\n",
    "    Context: E2E Networks is a cloud computing company that provides cloud infrastructure and cloud services to businesses and startups.\n",
    "    Unacceptable Answer: Based on the context, I think the pricing for E2E Networks is...\n",
    "    Acceptable Answer: The pricing for E2E Networks is...\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "# Create the LLM using the HuggingFaceLLM class\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=context_window,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=model_name,\n",
    "    model_name=model_name,\n",
    "    device_map=device,\n",
    "    generate_kwargs={\"temperature\": 0.2, \"top_k\": 5, \"top_p\": 0.95, \"do_sample\": True},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    # model_kwargs={\n",
    "    #     # \"torch_dtype\": torch.float16\n",
    "    #     'quantization_config':quantization_config\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"BAAI/bge-large-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "# Create the embedding model using the HuggingFaceBgeEmbeddings class\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceBgeEmbeddings(model_name=embedding_model_name)\n",
    ")\n",
    "\n",
    "# Get the embedding dimension of the model by doing a forward pass with a dummy input\n",
    "embed_dim = len(embed_model.get_text_embedding(\"Hello world\")) # 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"postgresql://postgres:test123@localhost:5432\"\n",
    "db_name = \"chatbotdb\"\n",
    "table_name = 'companyDocEmbeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "# Creates a URL object from the connection string\n",
    "url = make_url(connection_string)\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=table_name,\n",
    "    embed_dim=embed_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Load the index from the vector store of the database\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.vector_store.retrievers.retriever import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "# Create the retriever that manages the index and the number of results to return\n",
    "retriever = VectorIndexRetriever(\n",
    "      index=index,\n",
    "      similarity_top_k=16,\n",
    ")\n",
    "\n",
    "# Create the response synthesizer that will be used to synthesize the response\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "      response_mode='simple_summarize',\n",
    ")\n",
    "\n",
    "# Create the query engine that will be used to query the retriever and synthesize the response\n",
    "engine = RetrieverQueryEngine(\n",
    "      retriever=retriever,\n",
    "      response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E Networks is a cloud computing company that provides cloud infrastructure and cloud services to businesses and startups. They offer\n",
      "various solutions like API & CLI Access, CDN, Reserved IP, and Cloud Computing solutions. Their platform focuses on affordability,\n",
      "assistance, accessibility, accommodative, and AtmanirbharBharat (self-reliant India) principles. They have a reputation for being a trusted\n",
      "and reliable partner for Higher Education and Research Institutions, Enterprises, and AI/ML startups in India and globally.\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('what is the business of e2e networks?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E operates in the following countries: United States of America (USA), United Kingdom (UK), Germany, France, Canada, Switzerland,\n",
      "Australia, Netherlands, Italy, Spain, Israel, Sweden, Belgium, Austria, Singapore, Norway, Denmark, Finland, Portugal, Brazil, UAE, and\n",
      "South Korea.\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('In what all countries does E2E Operate?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E provides various GPU powered machines including NVIDIA T4, A100, and H100 GPUs.\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('What all GPU powered machines does E2E provide?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E provides GPU instances for high-end machine learning and deep learning tasks, starting as low as ₹ 30/hr for NVIDIA T4 GPUs. They also\n",
      "offer NVIDIA A100/H100 GPUs, which are the cloud's flagship machine learning platform, Tir. For more information, please visit the E2E\n",
      "Networks website. [Link](https://www.e2enetworks.com/products#cloud-GPUs)\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('What all machines does E2E provide for high-end machine/deep learning tasks?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price of HGX 8xH100 on E2E Cloud starts as low as ₹ 800/hr.\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('What is the price of HGX 8xH100?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E Networks has a Refund Policy that is published on their website. The policy covers various scenarios such as invoice discrepancies,\n",
      "minimum billing amounts, promotional code benefits, and software licenses. The policy also mentions that if you find any discrepancies in\n",
      "the invoices, you need to email them within 7 days of receiving the invoice. The policy also states that if you do not inform them of any\n",
      "discrepancy within 7 days, they will not entertain any requests to modify bills or offer refunds in any other form. For more details, you\n",
      "can refer to the Refund Policy published on the E2E Networks website. [Refund Policy](https://www.e2enetworks.com/policies/refund-policy)\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('What is the refund policy of E2E Networks?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We take privacy seriously and implement reasonable and appropriate measures designed to help secure your data against accidental or unlawful\n",
      "loss, access or disclosure. However, you are responsible for properly configuring and using the Services and maintaining the security and\n",
      "confidentiality of your accounts and access to the hosted systems, including encrypting any Personal Information you store on or transmit\n",
      "to/from our hosted system. We do not promise to retain any preservations or backups of your Customer Data. For more information, please\n",
      "refer to the Privacy Policy of the relevant customer to which you submitted your Personal Information.\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('What about privacy of my data?')\n",
    "print(wrapper.fill(response.response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
