{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to Building an AI Chatbot Using Mixtral 8x7B for SaaS Entrepreneurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's world, chatbots are becoming very popular. They are now-a-days integrated in almost every industry. They are being used in customer service, sales, marketing, and even in the healthcare industry. Chatbots are being used to automate repetitive tasks and to provide a better customer experience. SaaS industry which stands in the forefront of this AI revolution, constantly seeks to improve customer experience and chatbots are a great way to do that. With the advancements in AI and NLP, building a chatbot has never been more easier yet more powerful. There are many use cases of chatbots. If I start writing about all of them, it will probably take this whole blog. But let's see some of the most common use cases of chatbots. Chatbots are being used in: Q&A, customer support, lead generation, appointment scheduling, feedback collection, and many more. In this blog, we will see how to build an AI chatbot using Mixtral 8x7B model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre vs Post LLMs\n",
    "\n",
    "But it's not like chatbot didn't exist before the advancements of LLMs. Chatbots have been around for a long time. But the chatbots that were built before the advancements of LLMs were rule-based chatbots also known as decision-tree chatbots. These chatbots were built using a set of rules and if-else statements. They were not very intelligent and were not able to understand the context of the conversation. It all started in 1950s when Alan Turing proposed the Turing Test. The Turing Test is a test of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. Post this, there have been many attempts to build chatbots, including Eliza (1966), Parry (1972), Jabberwacky (1981), A.L.I.C.E (1995), IBM Watson (2006), and many more. Though these chatbots were able to perform some tasks, these were not very intelligent. Then came SIRI in 2011, which worked well. Still far from perfect, but it was a big step in the right direction. Post this there were many attempts to build chatbots powered by AI and NLP. \n",
    "\n",
    "But the real breakthrough came in 2019 when OpenAI released GPT-2. GPT-2 was a very powerful language model and was able to generate human-like text. Then came GPT-3 (2020) and ChatGPT (2022) which simply changed the game. These models were able to understand the context of the conversation and were able to generate human-like text. Since then, many SaaS companies have started using these models to build chatbots. In this blog, we will see exactly how to get this done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Chatbots in today's world\n",
    "Post ChatGPT, there are mainly three types of chatbots that are being used in today's world. These are: Vanilla LLMs as chatbot, RAG-Based Chatbots, and Fine-Tuned LLMs for a specific use case. Let's see what these are and their pros and cons.\n",
    "\n",
    "## Vanilla LLMs as chatbot\n",
    "ChatGPT when released in 2020 is a very powerful language model that has been in use since. People have been using it as a general-purpose chatbot. But soon researchers and developers started to realize that a general-purpose chatbot is not always the best solution if we want to build a chatbot for a specific use case. For example, if we want to build a chatbot for customer support, we don't need our chatbot to give information some random topic. We need our chatbot to focus on the use-case and topic at hand. In it's initial days, ChatGPT also suffered from knowledge restriction, since it was trained on a knowledge base prior to 2021. It also suffered from hallucination, which means it was generating text that was not relevant to the context of the conversation. So, though it was able to generate human-like text for general-purpose, it was not suitable for specific use-cases.\n",
    "\n",
    "## RAG-Based Chatbots\n",
    "Realizing the need for a chatbot that can understand the context of the conversation and can also provide relevant information from a knowledge base, researchers at Facebook AI developed RAG (Retrieval-Augmented Generation) model. RAG is a combination of a retrieval model and a generation model. The retrieval model is used to retrieve relevant information from a knowledge base and the generation model is used to generate human-like text. This model was able to understand the context of the conversation and was also able to provide relevant information from a knowledge base. This model was a big step in the right direction. There are types of RAG depending on the use case, some train the retrieval model and the generation model together, some rely on pre-trained retrieval model and generation model. But the main idea is the same.\n",
    "\n",
    "## Fine-Tuned LLMs for a specific use case\n",
    "Often, the RAG-based chatbots are not enough. At times, they are not able retrieve relevant information from the knowledge base and at times suffer from hallucination. This is where fine-tuned LLMs come into play. In this type of chatbot, we take a pre-trained LLM and fine-tune it on a specific use case. This is the most powerful and the most effective type of chatbot. This type of chatbot is able to understand the context of the conversation more clearly and is also able to generate responses that are more relevant to the use-case with minimal hallucination. \n",
    "\n",
    "In this blog, we will see how to build a RAG-based chatbot. Specifically, we will see how to build a chatbot using Mixtral 8x7B model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2E Chatbot\n",
    "\n",
    "In this blog we're going to build a RAG-based chatbot for a website to answer queries about the website in question. This chatbot will be able help use navigate the website, answer questions about the website, it's products, and services. It can also answer FAQs, Privacy policy about the website. Having a chatbot on the website can help the website owner to provide better customer experience and make the website more interactive and engaging.\n",
    "\n",
    "In this blog we're going to build this chatbot for E2E Networks. E2E Networks is a cloud computing company that provides cloud computing services to businesses. The chatbot will be able to answer questions about E2E Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Build!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in this blog, we will use Mixtral 8x7B model as the generation model (LLM), and we will use BAAI General Embedding (BGE) as retrieval model (Embeddings Model) to retrieve relevant information from a knowledge base. We're going to use PGVector from postgres as the database to store the knowledge base. We will use the Llama-Index to build this RAG Pipleine. Let's get right into it.\n",
    "\n",
    "This whole blog is divided into 3 parts:\n",
    "1. Indexing the knowledge base - We start by first collecting the data and then indexing it using Llama-Index and finally storing it in the database.\n",
    "2. Building the RAG Pipeline and Chatbot - We then build the RAG pipeline using Mixtral 8x7B and BAAI General Embedding, load the knowledge base from the database, and then build the chatbot.\n",
    "3. Finally, we wrap the chatbot in a websocket for real-time chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtral 8x7B\n",
    "\n",
    "Mixtral-8x7B, successor of Mistral-7B, is one of the latest released model from Mistral AI. It is a Mixture of Experts (MoE) model, which utilizes multiple experts with a router to decide their use, enhancing scalability and speed. It has mainly two components: Sparse MoE Layer and Gate Network or Router. The model can handle a context length of 32k tokens and supports multiple languages, including English, French, Italian, German, and Spanish. Performance-wise, Mixtral-8x7B surpasses Mistral-7B, while achieving similar or better results compared to GPT 3.5 and Llama 2 70B. Despite its 46.7B parameters, only around 12.9B are utilized during inference. Additionally, Mistral AI has released the Mixtral-8x7B-Instruct model alongside the foundation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAAI General Embedding (BGE)\n",
    "\n",
    "Like we discussed earlier in first part, We're going to use BAAI General Embedding (BGE) as retrieval model (Embeddings Model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@vivekpatil647/timeline-of-chatbots-f3baf14c05e6\n",
    "\n",
    "https://arxiv.org/pdf/2005.11401v4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the HuggingFace class from the llama-index to build the LLM. We set the context length to 4096, while limiting the maximum number of new tokens to be generated to 256. While building the LLM we also need to provide it with system prompt. The system prompt is the prompt that will define the behavior of the LLM. A lot of effort went into fine tuning the system prompt to make the LLM more suitable for the use case and avoid it from giving irrelevant information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86fa2f9693449409a7a1cf3601c8746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "# from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# Context Window specifies how many tokens to use as context for the LLM\n",
    "context_window = 4096\n",
    "# Max New Tokens specifies how many new tokens to generate for the LLM\n",
    "max_new_tokens = 256\n",
    "# Device specifies which device to use for the LLM\n",
    "device = \"cuda\"\n",
    "\n",
    "# This is the prompt that will be used to instruct the model behavior\n",
    "system_prompt = \"\"\"\n",
    "    You are an AI chatbot that is designed to answer questions related to E2E Networks. \n",
    "    You are provided with a context and a question. You need to answer the question based on the context provided. \n",
    "    If the context is not helpful, do not answer based on prior knowledge, instead, redirect the user to the E2E Networks Support team. \n",
    "    You should also provide links that you got from context that are relevant to the answer. \n",
    "    You are allowed to answer in first person only, like I/Us/Our; It should feel like a human is answering the question. \n",
    "    You should only provide the links and not like [E2E Networks Official Website](https://www.e2enetworks.com/)\n",
    "    You're not allowed to say something like \"Based on the context, I think the answer is...\", instead, you should directly answer the question.\n",
    "    When in confusion, you can ask for more information from the user.\n",
    "\n",
    "    Here is an example of how you should answer:\n",
    "\n",
    "    Question: What is the pricing for E2E Networks?\n",
    "    Context: E2E Networks is a cloud computing company that provides cloud infrastructure and cloud services to businesses and startups.\n",
    "    Unacceptable Answer: Based on the context, I think the pricing for E2E Networks is...\n",
    "    Acceptable Answer: The pricing for E2E Networks is...\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "# Create the LLM using the HuggingFaceLLM class\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=context_window,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=model_name,\n",
    "    model_name=model_name,\n",
    "    device_map=device,\n",
    "    generate_kwargs={\"temperature\": 0.2, \"top_k\": 5, \"top_p\": 0.95, \"do_sample\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're going to use the BAAI General Embedding (BGE) model as the retrieval model. We will use the HuggingFaceBgeEmbedding class from the langchain library to build the BGE model. We will use the BGE model to retrieve relevant information from the knowledge base. We also require the embedding dimension, which is the dimension of the vector representation of the text data. We can either get this information from the HuggingFace website or we can simply do a forward pass of the model to get the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"BAAI/bge-large-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "# Create the embedding model using the HuggingFaceBgeEmbeddings class\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceBgeEmbeddings(model_name=embedding_model_name)\n",
    ")\n",
    "\n",
    "# Get the embedding dimension of the model by doing a forward pass with a dummy input\n",
    "embed_dim = len(embed_model.get_text_embedding(\"Hello world\")) # 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like earlier, we will use the settings class from the llama-index to set the settings for the LLM and the BGE model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like earlier, we're going to setup the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"postgresql://postgres:test123@localhost:5432\"\n",
    "db_name = \"chatbotdb\"\n",
    "table_name = 'companyDocEmbeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "# Creates a URL object from the connection string\n",
    "url = make_url(connection_string)\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=table_name,\n",
    "    embed_dim=embed_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the index of the knowledge base from the database, that we stored in the first part, in order to build the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Load the index from the vector store of the database\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally setup our chatbot engine using the index from the database we just loaded in our memory. There can be several ways we can do this, with simplest approach being using the index to create a simple engine that can be used to query the knowledge base. But we're going to customize the engine a bit to make it more suitable for our use case. We are going to extract 16 relevant chunks from the knowledge base for a given query. Since we're using the HTML directly from the E2E website instead of a PDF or text document, some information might not correctly convert to text when doing the conversion of HTML to text. Also, lot of times, in website same information is present in multiple pages, and we want to take this to our advantage. So, we're going to extract 16 relevant chunks from the knowledge base for a given query. And then finally do a simple summarization of the relevant chunks to get the final response. This way even of some information is not correctly formatted in the text, we can still get the relevant information from the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.vector_store.retrievers.retriever import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "# Create the retriever that manages the index and the number of results to return\n",
    "retriever = VectorIndexRetriever(\n",
    "      index=index,\n",
    "      similarity_top_k=16,\n",
    ")\n",
    "\n",
    "# Create the response synthesizer that will be used to synthesize the response\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "      response_mode='simple_summarize',\n",
    ")\n",
    "\n",
    "# Create the query engine that will be used to query the retriever and synthesize the response\n",
    "engine = RetrieverQueryEngine(\n",
    "      retriever=retriever,\n",
    "      response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E Networks is a cloud computing company that provides cloud infrastructure and cloud services to businesses and startups. They offer\n",
      "various solutions like API & CLI Access, CDN, Reserved IP, and Cloud Computing solutions. Their platform focuses on affordability,\n",
      "assistance, accessibility, accommodative, and AtmanirbharBharat (self-reliant India) principles. They have a reputation for being a trusted\n",
      "and reliable partner for Higher Education and Research Institutions, Enterprises, and AI/ML startups in India and globally.\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('what is the business of e2e networks?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E operates in the following countries: United States of America (USA), United Kingdom (UK), Germany, France, Canada, Switzerland,\n",
      "Australia, Netherlands, Italy, Spain, Israel, Sweden, Belgium, Austria, Singapore, Norway, Denmark, Finland, Portugal, Brazil, UAE, and\n",
      "South Korea.\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('In what all countries does E2E Operate?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E provides various GPU powered machines including NVIDIA T4, A100, and H100 GPUs.\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('What all GPU powered machines does E2E provide?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E provides GPU instances for high-end machine learning and deep learning tasks, starting as low as ₹ 30/hr for NVIDIA T4 GPUs. They also\n",
      "offer NVIDIA A100/H100 GPUs, which are the cloud's flagship machine learning platform, Tir. For more information, please visit the E2E\n",
      "Networks website. [Link](https://www.e2enetworks.com/products#cloud-GPUs)\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('What all machines does E2E provide for high-end machine/deep learning tasks?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price of HGX 8xH100 on E2E Cloud starts as low as ₹ 800/hr.\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('What is the price of HGX 8xH100?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E Networks has a Refund Policy that is published on their website. The policy covers various scenarios such as invoice discrepancies,\n",
      "minimum billing amounts, promotional code benefits, and software licenses. The policy also mentions that if you find any discrepancies in\n",
      "the invoices, you need to email them within 7 days of receiving the invoice. The policy also states that if you do not inform them of any\n",
      "discrepancy within 7 days, they will not entertain any requests to modify bills or offer refunds in any other form. For more details, you\n",
      "can refer to the Refund Policy published on the E2E Networks website. [Refund Policy](https://www.e2enetworks.com/policies/refund-policy)\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('What is the refund policy of E2E Networks?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We take privacy seriously and implement reasonable and appropriate measures designed to help secure your data against accidental or unlawful\n",
      "loss, access or disclosure. However, you are responsible for properly configuring and using the Services and maintaining the security and\n",
      "confidentiality of your accounts and access to the hosted systems, including encrypting any Personal Information you store on or transmit\n",
      "to/from our hosted system. We do not promise to retain any preservations or backups of your Customer Data. For more information, please\n",
      "refer to the Privacy Policy of the relevant customer to which you submitted your Personal Information.\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('What about privacy of my data?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I can provide you with the contact details of the sales team at E2E Networks. You can reach out to them at\n",
      "[sales@e2enetworks.com](mailto:sales@e2enetworks.com) or call them at +91-11-4084-4965. Their working hours are from 10:00 am to 8:00 pm\n",
      "(Monday to Saturday).\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('Can you provide me the contact details of sales teams of E2E Networks?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The E2E Networks Office is located at Awfis, First Floor, A-24/9, Mohan Cooperative Industrial Estate, Mathura Road, Saidabad, New\n",
      "Delhi-110044. In Bengaluru, the office is at Urban Vault, Indiranagar: No. 3/1-a-l Doopanahalli Grama, 16th Main Rd, HAL 2nd Stage,\n",
      "Kodihalli, Bengaluru, Karnataka-560008. And in Mumbai, the office is at Plot No. D-5 Road No. 20, Marol MIDC, 91 Springboard Business Hub\n",
      "Private Limited, Andheri East, Mumbai, Maharashtra-400093.\n"
     ]
    }
   ],
   "source": [
    "response = engine.query('Can you provide me the address of the E2E Networks Office?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
