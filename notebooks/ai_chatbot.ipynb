{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.llms import ChatMessage, MessageRole\n",
    "# from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "# # Text QA Prompt\n",
    "# chat_text_qa_msgs = [\n",
    "#     ChatMessage(\n",
    "#         role=MessageRole.SYSTEM,\n",
    "#         content=(\n",
    "#             \"Always answer the question, even if the context isn't helpful.\"\n",
    "#         ),\n",
    "#     ),\n",
    "#     ChatMessage(\n",
    "#         role=MessageRole.USER,\n",
    "#         content=(\n",
    "#             \"Context information is below.\\n\"\n",
    "#             \"---------------------\\n\"\n",
    "#             \"{context_str}\\n\"\n",
    "#             \"---------------------\\n\"\n",
    "#             \"Given the context information and not prior knowledge, \"\n",
    "#             \"answer the question: {query_str}\\n\"\n",
    "#         ),\n",
    "#     ),\n",
    "# ]\n",
    "# text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n",
    "\n",
    "# # Refine Prompt\n",
    "# chat_refine_msgs = [\n",
    "#     ChatMessage(\n",
    "#         role=MessageRole.SYSTEM,\n",
    "#         content=(\n",
    "#             \"Always answer the question, even if the context isn't helpful.\"\n",
    "#         ),\n",
    "#     ),\n",
    "#     ChatMessage(\n",
    "#         role=MessageRole.USER,\n",
    "#         content=(\n",
    "#             \"We have the opportunity to refine the original answer \"\n",
    "#             \"(only if needed) with some more context below.\\n\"\n",
    "#             \"------------\\n\"\n",
    "#             \"{context_msg}\\n\"\n",
    "#             \"------------\\n\"\n",
    "#             \"Given the new context, refine the original answer to better \"\n",
    "#             \"answer the question: {query_str}. \"\n",
    "#             \"If the context isn't useful, output the original answer again.\\n\"\n",
    "#             \"Original Answer: {existing_answer}\"\n",
    "#         ),\n",
    "#     ),\n",
    "# ]\n",
    "# refine_template = ChatPromptTemplate(chat_refine_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8d306756bc4d38b1b80bb82c82c1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "# from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# Context Window specifies how many tokens to use as context for the LLM\n",
    "context_window = 2048\n",
    "# Max New Tokens specifies how many new tokens to generate for the LLM\n",
    "max_new_tokens = 256\n",
    "# Device specifies which device to use for the LLM\n",
    "device = \"cuda\"\n",
    "\n",
    "# This is the prompt that will be used to instruct the model behavior\n",
    "system_prompt = \"\"\"\n",
    "    You are an AI chatbot that is designed to answer questions related to E2E Networks. \n",
    "    You are provided with a context and a question. You need to answer the question based on the context provided. \n",
    "    If the context is not helpful, do not answer based on prior knowledge, instead, redirect the user to the E2E Networks Support team. \n",
    "    You should also provide links that you got from context that are relevant to the answer. \n",
    "    You are allowed to answer in first person only, like I/Us/Our; It should feel like a human is answering the question. \n",
    "    You only provide the like and not like [E2E Networks Official Website](https://www.e2enetworks.com/)\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "# Create the LLM using the HuggingFaceLLM class\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=context_window,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=PromptTemplate(\"<s>[INST] {query_str} [/INST] </s>\\n\"),\n",
    "    tokenizer_name=model_name,\n",
    "    model_name=model_name,\n",
    "    device_map=device,\n",
    "    generate_kwargs={\"temperature\": 0.2, \"top_k\": 5, \"top_p\": 0.95},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    # model_kwargs={\n",
    "    #     # \"torch_dtype\": torch.float16\n",
    "    #     'quantization_config':quantization_config\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"BAAI/bge-large-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "# Create the embedding model using the HuggingFaceBgeEmbeddings class\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceBgeEmbeddings(model_name=embedding_model_name)\n",
    ")\n",
    "\n",
    "# Get the embedding dimension of the model by doing a forward pass with a dummy input\n",
    "embed_dim = len(embed_model.get_text_embedding(\"Hello world\")) # 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"postgresql://postgres:test123@localhost:5432\"\n",
    "db_name = \"chatbotdb\"\n",
    "table_name = 'companyDocEmbeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "Settings.chunk_size = 2048\n",
    "Settings.chunk_overlap = 256\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.transformations = [SentenceSplitter(chunk_size=1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "# Creates a URL object from the connection string\n",
    "url = make_url(connection_string)\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=table_name,\n",
    "    embed_dim=embed_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import SummaryIndex\n",
    "\n",
    "# Load the index from the vector store of the database\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "# summary_index = SummaryIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.vector_store.retrievers.retriever import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "# Create the retriever that manages the index and the number of results to return\n",
    "retriever = VectorIndexRetriever(\n",
    "      index=index,\n",
    "      similarity_top_k=5,\n",
    ")\n",
    "\n",
    "# Create the response synthesizer that will be used to synthesize the response\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "      response_mode='simple_summarize',\n",
    ")\n",
    "\n",
    "# Create the query engine that will be used to query the retriever and synthesize the response\n",
    "query_engine = RetrieverQueryEngine(\n",
    "      retriever=retriever,\n",
    "      response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepkapha/anaconda3/envs/ai-chatbot/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/deepkapha/anaconda3/envs/ai-chatbot/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/deepkapha/anaconda3/envs/ai-chatbot/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('Do you know about e2e networks? What is e2e networks? And what all services do they provide?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an AI chatbot designed to answer questions related to E2E Networks based on the context provided. According to the context, E2E Networks\n",
      "Limited is a rapidly growing cloud computing player based in India. They offer various cloud services including Webuzo Linux Cloud, High\n",
      "Memory Cloud, E2E Object Storage, E2E Volumes, and Windows Cloud. The Webuzo Linux Cloud provides fully-featured Multi-Account Webuzo\n",
      "Control panel with plans that include varying numbers of vCPUs, CPU frequency, dedicated RAM, and disk space. The High Memory Cloud offers\n",
      "access to in-memory data faster than latest NVMe flash storage and is ideal for memory-intensive workloads. E2E Object Storage is an SSD-\n",
      "based S3-compatible object storage service designed for demanding workloads like machine learning and deep learning. E2E Volumes provides\n",
      "block-level storage, and Windows Cloud offers Microsoft Windows Server 2016/2019 on E2E Networks Cloud for Windows Server workloads. For\n",
      "more information, you can visit the E2E Networks website or contact their sales team at [sales@e2enetworks.com](mailto:sales@\n"
     ]
    }
   ],
   "source": [
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is the price of HGX 8xH100?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry for any confusion, but the context provided does not mention the price of the HGX 8xH100 GPU. The context only provides\n",
      "information about the H100 GPU and its different plans with varying vCPUs, dedicated RAM, and disk space, along with their hourly, weekly,\n",
      "monthly, and longer-term billing options. If you have any other questions related to the context or E2E Networks in general, feel free to\n",
      "ask and I'll do my best to help you out. If you need more specific pricing information for the HGX 8xH100, I would recommend reaching out to\n",
      "E2E Networks' sales team at [+91-11-4084-4965](callto:%2B91-11-4084-4965) or [sales@e2enetworks.com](mailto:sales@e2enetworks.com). They\n",
      "would be able to provide you with the most accurate and up-to-date pricing information.  Here's the link to the E2E Networks Support team:\n",
      "[https://www.\n"
     ]
    }
   ],
   "source": [
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, E2E Networks offers CDP (Continuous Data Protection) backup plans for servers. However, it's important to\n",
      "note that maintaining your own backups is also an option. If you have signed up for a backup plan from E2E Networks, they will backup your\n",
      "entire server minus excluded directories as per a backup schedule. If you prefer using a third-party backup service, you can choose from\n",
      "options like Tarsnap, Barracuda, Rackspace, or Zmanda Cloud Backup.  For more information about E2E Networks CDP backup, you can visit their\n",
      "website or contact their sales team at [+91-11-4084-4965](callto:%2B91-11-4084-4965) or\n",
      "[sales@e2enetworks.com](mailto:sales@e2enetworks.com).  Here's the link to the E2E CDP Backup page on their website: [E2E CDP\n",
      "Backup](https://www.e2enetworks.com/product/e2e-cd\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('Is my server at E2E Networks automatically backed up?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "vector_tool = QueryEngineTool(\n",
    "    index.as_query_engine(),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Useful for searching for specific facts.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "\n",
    "query_engine = RouterQueryEngine.from_defaults(\n",
    "    [vector_tool],\n",
    "    select_multi=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, E2E Networks offers CDP (Continuous Data Protection) backup plans for servers, but the billing for Microsoft\n",
      "licenses is on a calendar month basis, regardless of usage. If you deprovision your private cloud server in the middle of a billing period,\n",
      "you will receive a prorated refund, but not for the first month's service fee. E2E Networks provides a 99.95% uptime, and you can customize\n",
      "your cloud servers by choosing from the available options in the public cloud. The internet bandwidth is not metered, but in exceptional\n",
      "cases, dedicated bandwidth can be provided for an additional cost. You get root access to your server and can host multiple websites\n",
      "depending on the resources in your Cloud server plan. The server provisioning process takes around 5-10 minutes once the payment is\n",
      "completed.  Regarding your question, E2E Networks does offer CDP backup plans for servers, but the context does not provide information\n",
      "about whether these backups are included in the monthly billing for Microsoft licenses or if they are an additional cost. If you have signed\n",
      "up for a backup plan, your server will be backed up as per the schedule you\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('Is my server at E2E Networks automatically backed up?')\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry for any confusion, but the context provided does not include information about the price of an 8xH100 configuration. The context\n",
      "only mentions the prices for L40S, 2xL40S, and 4xL40S configurations. If you have more specific details about the hardware or plan you're\n",
      "interested in, please let me know and I'll try to help you find that information. In the meantime, you can contact E2E Networks sales team\n",
      "at [+91-11-4084-4965](callto:%2B91-11-4084-4965) or [sales@e2enetworks.com](mailto:sales@e2enetworks.com) for the most accurate and up-to-\n",
      "date pricing information.  Here are some links that might be helpful for you:  * [E2E Networks L40S Cloud\n",
      "GPU](https://www.e2enetworks.com/product/l40s-cloud-gpu) * [E2E Networks NVIDIA A100\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"'What is the price of 8xH100?'\")\n",
    "print(wrapper.fill(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
