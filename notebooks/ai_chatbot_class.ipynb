{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.core import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core import Settings\n",
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.vector_store.retrievers.retriever import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "embedding_model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "# Context Window specifies how many tokens to use as context for the LLM\n",
    "context_window = 4096\n",
    "# Max New Tokens specifies how many new tokens to generate for the LLM\n",
    "max_new_tokens = 256\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 256\n",
    "# Device specifies which device to use for the LLM\n",
    "device = \"cuda\"\n",
    "connection_string = \"postgresql://postgres:test123@localhost:5432\"\n",
    "db_name = \"chatbotdb\"\n",
    "table_name = 'companyDocEmbeddings'\n",
    "# This is the prompt that will be used to instruct the model behavior\n",
    "system_prompt = \"\"\"\n",
    "    You are an AI chatbot that is designed to answer questions related to E2E Networks. \n",
    "    You are provided with a context and a question. You need to answer the question based on the context provided. \n",
    "    If the context is not helpful, do not answer based on prior knowledge, instead, redirect the user to the E2E Networks Support team. \n",
    "    You should also provide links that you got from context that are relevant to the answer. \n",
    "    You are allowed to answer in first person only, like I/Us/Our; It should feel like a human is answering the question. \n",
    "    You should only provide the links and not like [E2E Networks Official Website](https://www.e2enetworks.com/)\n",
    "    You're not allowed to say something like \"Based on the context, I think the answer is...\", instead, you should directly answer the question.\n",
    "    When in confusion, you can ask for more information from the user.\n",
    "\n",
    "    Here is an example of how you should answer:\n",
    "\n",
    "    Question: What is the pricing for E2E Networks?\n",
    "    Context: E2E Networks is a cloud computing company that provides cloud infrastructure and cloud services to businesses and startups.\n",
    "    Unacceptable Answer: Based on the context, I think the pricing for E2E Networks is...\n",
    "    Acceptable Answer: The pricing for E2E Networks is...\n",
    "\"\"\"\n",
    "top_k_index_to_return = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIChatBot:\n",
    "    def __init__(self, model_name, embedding_model_name, system_prompt, database_connection_string, \n",
    "    database_name, table_name, context_window, max_new_tokens, device, chunk_size, chunk_overlap, top_k_index_to_return):\n",
    "        # Initialize model configuration and database connection here\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.database_connection_string = database_connection_string\n",
    "        self.database_name = database_name\n",
    "        self.table_name = table_name\n",
    "        self.system_prompt = system_prompt\n",
    "        self.context_window = context_window\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.device = device\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.top_k_index_to_return = top_k_index_to_return\n",
    "\n",
    "    def setup_model(self):\n",
    "        # This will wrap the default prompts that are internal to llama-index\n",
    "        query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "        # Create the LLM using the HuggingFaceLLM class\n",
    "        llm = HuggingFaceLLM(\n",
    "            context_window=self.context_window,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            system_prompt=self.system_prompt,\n",
    "            query_wrapper_prompt=query_wrapper_prompt,\n",
    "            tokenizer_name=self.model_name,\n",
    "            model_name=self.model_name,\n",
    "            device_map=self.device,\n",
    "            generate_kwargs={\"temperature\": 0.2, \"top_k\": 5, \"top_p\": 0.95, \"do_sample\": True},\n",
    "            # uncomment this if using CUDA to reduce memory usage\n",
    "            # model_kwargs={\n",
    "            #     # \"torch_dtype\": torch.float16\n",
    "            #     'quantization_config':quantization_config\n",
    "            # }\n",
    "        )\n",
    "\n",
    "        # Create the embedding model using the HuggingFaceBgeEmbeddings class\n",
    "        embed_model = LangchainEmbedding(\n",
    "        HuggingFaceBgeEmbeddings(model_name=embedding_model_name)\n",
    "        )\n",
    "\n",
    "        # Get the embedding dimension of the model by doing a forward pass with a dummy input\n",
    "        embed_dim = len(embed_model.get_text_embedding(\"Hello world\")) # 1024\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.llm = llm\n",
    "        self.embed_model = embed_model\n",
    "\n",
    "    def apply_settings(self):\n",
    "        Settings.llm = self.llm\n",
    "        Settings.embed_model = self.embed_model\n",
    "\n",
    "\n",
    "        Settings.chunk_size = self.chunk_size\n",
    "        Settings.chunk_overlap = self.chunk_overlap\n",
    "\n",
    "    def get_index_from_database(self):\n",
    "        # Creates a URL object from the connection string\n",
    "        url = make_url(self.database_connection_string)\n",
    "\n",
    "        # Create the vector store\n",
    "        vector_store = PGVectorStore.from_params(\n",
    "            database=self.database_name,\n",
    "            host=url.host,\n",
    "            password=url.password,\n",
    "            port=url.port,\n",
    "            user=url.username,\n",
    "            table_name=self.table_name,\n",
    "            embed_dim=self.embed_dim,\n",
    "        )\n",
    "\n",
    "        # Load the index from the vector store of the database\n",
    "        index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "        self.index = index\n",
    "\n",
    "    def setup_engine(self):\n",
    "        # Create the retriever that manages the index and the number of results to return\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=self.index,\n",
    "            similarity_top_k=self.top_k_index_to_return,\n",
    "        )\n",
    "\n",
    "        # Create the response synthesizer that will be used to synthesize the response\n",
    "        response_synthesizer = get_response_synthesizer(\n",
    "            response_mode='simple_summarize',\n",
    "        )\n",
    "\n",
    "        # Create the query engine that will be used to query the retriever and synthesize the response\n",
    "        engine = RetrieverQueryEngine(\n",
    "            retriever=retriever,\n",
    "            response_synthesizer=response_synthesizer,\n",
    "        )\n",
    "        self.engine = engine\n",
    "\n",
    "    def build_bot(self):\n",
    "        self.setup_model()\n",
    "        self.apply_settings()\n",
    "        self.get_index_from_database()\n",
    "        self.setup_engine()\n",
    "\n",
    "\n",
    "    def process_query(self, query_text):\n",
    "        response = self.engine.query(query_text)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E2E Networks is a cloud computing company that provides cloud infrastructure and cloud services to businesses and startups.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2e_chatbot = AIChatBot(model_name, embedding_model_name, system_prompt, connection_string, db_name, \n",
    "                        table_name, context_window, max_new_tokens, device, chunk_size, chunk_overlap, top_k_index_to_return)\n",
    "e2e_chatbot.build_bot()\n",
    "response = e2e_chatbot.process_query(\"What is the business of E2E Networks?\")\n",
    "response.response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
